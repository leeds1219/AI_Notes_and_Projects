{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leeds1219/AI_Notes_and_Projects/blob/main/Single_Shot_Multibox_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MXNet Install\n",
        "!pip install mxnet-cu112  # Check compatibility with your CUDA version\n",
        "\n",
        "# Google Drive Mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')  # Mount Google Drive\n",
        "\n",
        "# Change directory to Colab Notebooks\n",
        "%cd '/content/drive/MyDrive/Colab Notebooks'\n",
        "\n",
        "# Clone the GitHub repository\n",
        "!git clone https://github.com/MLman/d2l-pytorch.git\n",
        "\n",
        "# Change directory to the cloned repository\n",
        "%cd '/content/drive/MyDrive/Colab Notebooks/d2l-pytorch/d2l'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZZTy7As-Pc_",
        "outputId": "6ed5b19e-516e-4825-84a0-ab0e7e115bb5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mxnet-cu112\n",
            "  Downloading mxnet_cu112-1.9.1-py3-none-manylinux2014_x86_64.whl (499.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.4/499.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet-cu112) (1.23.5)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet-cu112) (2.31.0)\n",
            "Collecting graphviz<0.9.0,>=0.8.1 (from mxnet-cu112)\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-cu112) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-cu112) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-cu112) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-cu112) (2023.7.22)\n",
            "Installing collected packages: graphviz, mxnet-cu112\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.20.1\n",
            "    Uninstalling graphviz-0.20.1:\n",
            "      Successfully uninstalled graphviz-0.20.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-cu112-1.9.1\n",
            "Mounted at /content/drive/\n",
            "/content/drive/MyDrive/Colab Notebooks\n",
            "fatal: destination path 'd2l-pytorch' already exists and is not an empty directory.\n",
            "/content/drive/MyDrive/Colab Notebooks/d2l-pytorch/d2l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR1hnxOM-OL7"
      },
      "source": [
        "# Single Shot Multibox Detection (SSD)\n",
        "\n",
        "In the previous few sections, we have introduced bounding boxes, anchor boxes,\n",
        "multiscale object detection, and data sets. Now, we will use this background\n",
        "knowledge to construct an object detection model: single shot multibox detection\n",
        "(SSD) :cite:`Liu.Anguelov.Erhan.ea.2016`. This quick and easy model is already\n",
        "widely used. Some of the design concepts and implementation details of this\n",
        "model are also applicable to other object detection models.\n",
        "\n",
        "\n",
        "## Model\n",
        "\n",
        ":numref:`fig_ssd` shows the design of an SSD model. The model's main components\n",
        "are a base network block and several multiscale feature blocks connected in a\n",
        "series. Here, the base network block is used to extract features of original\n",
        "images, and it generally takes the form of a deep convolutional neural\n",
        "network. The paper on SSDs chooses to place a truncated VGG before the\n",
        "classification layer :cite:`Liu.Anguelov.Erhan.ea.2016`, but this is now\n",
        "commonly replaced by ResNet. We can design the base network so that it outputs\n",
        "larger heights and widths. In this way, more anchor boxes are generated based on\n",
        "this feature map, allowing us to detect smaller objects. Next, each multiscale\n",
        "feature block reduces the height and width of the feature map provided by the\n",
        "previous layer (for example, it may reduce the sizes by half). The blocks then\n",
        "use each element in the feature map to expand the receptive field on the input\n",
        "image. In this way, the closer a multiscale feature block is to the top of\n",
        ":numref:`fig_ssd` the smaller its output feature map, and the fewer the anchor\n",
        "boxes that are generated based on the feature map. In addition, the closer a\n",
        "feature block is to the top, the larger the receptive field of each element in\n",
        "the feature map and the better suited it is to detect larger objects. As the SSD\n",
        "generates different numbers of anchor boxes of different sizes based on the base\n",
        "network block and each multiscale feature block and then predicts the categories\n",
        "and offsets (i.e., predicted bounding boxes) of the anchor boxes in order to\n",
        "detect objects of different sizes, SSD is a multiscale object detection model.\n",
        "\n",
        "![The SSD is composed of a base network block and several multiscale feature blocks connected in a series. ](../img/ssd.svg)\n",
        "\n",
        ":label:`fig_ssd`\n",
        "\n",
        "\n",
        "\n",
        "Next, we will describe the implementation of the modules in :numref:`fig_ssd`. First, we need to discuss the implementation of category prediction and bounding box prediction.\n",
        "\n",
        "### Category Prediction Layer\n",
        "\n",
        "Set the number of object categories to $q$. In this case, the number of anchor\n",
        "box categories is $q+1$, with 0 indicating an anchor box that only contains\n",
        "background. For a certain scale, set the height and width of the feature map to\n",
        "$h$ and $w$, respectively. If we use each element as the center to generate $a$\n",
        "anchor boxes, we need to classify a total of $hwa$ anchor boxes. If we use a\n",
        "fully connected layer (FCN) for the output, this will likely result in an\n",
        "excessive number of model parameters. Recall how we used convolutional layer\n",
        "channels to output category predictions in :numref:`chapter_nin`. SSD uses the\n",
        "same method to reduce the model complexity.\n",
        "\n",
        "Specifically, the category prediction layer uses a convolutional layer that\n",
        "maintains the input height and width. Thus, the output and input have a\n",
        "one-to-one correspondence to the spatial coordinates along the width and height\n",
        "of the feature map. Assuming that the output and input have the same spatial\n",
        "coordinates $(x,y)$, the channel for the coordinates $(x,y)$ on the output\n",
        "feature map contains the category predictions for all anchor boxes generated\n",
        "using the input feature map coordinates $(x,y)$ as the center. Therefore, there\n",
        "are $a(q+1)$ output channels, with the output channels indexed as $i(q+1) + j$\n",
        "($0 \\leq j \\leq q$) representing the predictions of the category index $j$ for\n",
        "the anchor box index $i$.\n",
        "\n",
        "Now, we will define a category prediction layer of this type. After we specify\n",
        "the parameters $a$ and $q$, it uses a $3\\times3$ convolutional layer with a\n",
        "padding of 1. The heights and widths of the input and output of this\n",
        "convolutional layer remain unchanged."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6WBgs7dB-OL_"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "import d2l\n",
        "# from d2l.ssd_utils import *\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import json\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "def cls_predictor(input_channels, num_anchors, num_classes):\n",
        "    return nn.Conv2d(in_channels=input_channels, out_channels=num_anchors * (num_classes + 1), kernel_size=3,\n",
        "                     padding=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IBTrCWM-OMA"
      },
      "source": [
        "### Bounding Box Prediction Layer\n",
        "\n",
        "The design of the bounding box prediction layer is similar to that of the category prediction layer. The only difference is that, here, we need to predict 4 offsets for each anchor box, rather than $q+1$ categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Z3EumXdB-OMB"
      },
      "outputs": [],
      "source": [
        "def bbox_predictor(input_channels, num_anchors):\n",
        "    return nn.Conv2d(in_channels=input_channels, out_channels=num_anchors * 4, kernel_size=3, padding=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk__cYAU-OMB"
      },
      "source": [
        "### Concatenating Predictions for Multiple Scales\n",
        "\n",
        "As we mentioned, SSD uses feature maps based on multiple scales to generate anchor boxes and predict their categories and offsets. Because the shapes and number of anchor boxes centered on the same element differ for the feature maps of different scales, the prediction outputs at different scales may have different shapes.\n",
        "\n",
        "In the following example, we use the same batch of data to construct feature maps of two different scales, `Y1` and `Y2`. Here, `Y2` has half the height and half the width of `Y1`. Using category prediction as an example, we assume that each element in the `Y1` and `Y2` feature maps generates five (Y1) or three (Y2) anchor boxes. When there are 10 object categories, the number of category prediction output channels is either $5\\times(10+1)=55$ or $3\\times(10+1)=33$. The format of the prediction output is (batch size, number of channels, height, width). As you can see, except for the batch size, the sizes of the other dimensions are different. Therefore, we must transform them into a consistent format and concatenate the predictions of the multiple scales to facilitate subsequent computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXCS96im-OMB",
        "outputId": "a4f1a518-237a-4bcd-d98e-944b8768896a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 55, 20, 20]), torch.Size([2, 33, 10, 10]))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "def forward(x, block):\n",
        "    return block(x)\n",
        "Y1 = forward(torch.zeros((2, 8, 20, 20)), cls_predictor(8, 5, 10))\n",
        "Y2 = forward(torch.zeros((2, 16, 10, 10)), cls_predictor(16, 3, 10))\n",
        "(Y1.shape, Y2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97v3kMAW-OMC"
      },
      "source": [
        "The channel dimension contains the predictions for all anchor boxes with the same center. We first move the channel dimension to the final dimension. Because the batch size is the same for all scales, we can convert the prediction results to binary format (batch size, height $\\times$ width $\\times$ number of channels) to facilitate subsequent concatenation on the 1st dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "a06rf0Gp-OMC"
      },
      "outputs": [],
      "source": [
        "def flatten_pred(pred):\n",
        "    return pred.permute(0, 2, 3, 1).reshape(pred.size(0),-1)\n",
        "\n",
        "def concat_preds(preds):\n",
        "    return torch.cat(tuple([flatten_pred(p) for p in preds]), dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j42rPTYv-OMD"
      },
      "source": [
        "Thus, regardless of the different shapes of `Y1` and `Y2`, we can still concatenate the prediction results for the two different scales of the same batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnt6C3BB-OMD",
        "outputId": "61cef39b-0ccc-4ca3-a5ef-1767a89b498c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 25300])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "concat_preds([Y1, Y2]).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa4j6YSH-OMD"
      },
      "source": [
        "### Height and Width Downsample Block\n",
        "\n",
        "For multiscale object detection, we define the following `down_sample_blk` block, which reduces the height and width by 50%. This block consists of two $3\\times3$ convolutional layers with a padding of 1 and a $2\\times2$ maximum pooling layer with a stride of 2 connected in a series. As we know, $3\\times3$ convolutional layers with a padding of 1 do not change the shape of feature maps. However, the subsequent pooling layer directly reduces the size of the feature map by half. Because $1\\times 2+(3-1)+(3-1)=6$, each element in the output feature map has a receptive field on the input feature map of the shape $6\\times6$. As you can see, the height and width downsample block enlarges the receptive field of each element in the output feature map."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ESkd0OB_-OMD"
      },
      "outputs": [],
      "source": [
        "def down_sample_blk(input_channels, num_channels):\n",
        "    blk = []\n",
        "    for _ in range(2):\n",
        "        blk.append(nn.Conv2d(in_channels=input_channels, out_channels=num_channels, kernel_size=3, padding=1))\n",
        "        blk.append(nn.BatchNorm2d(num_features=num_channels))\n",
        "        blk.append(nn.ReLU())\n",
        "        input_channels=num_channels\n",
        "    blk.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "    blk = nn.Sequential(*blk)\n",
        "    return blk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuHY7IwF-OME"
      },
      "source": [
        "By testing forward computation in the height and width downsample block, we can see that it changes the number of input channels and halves the height and width."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_2Z9TQ--OME",
        "outputId": "eda33119-8b73-4f01-cf76-b62631743ed1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 10, 10, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "forward(torch.zeros((2, 3, 20, 20)), down_sample_blk(3, 10)).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbMV6OCs-OME"
      },
      "source": [
        "### Base Network Block\n",
        "\n",
        "The base network block is used to extract features from original images. To simplify the computation, we will construct a small base network. This network consists of three height and width downsample blocks connected in a series, so it doubles the number of channels at each step. When we input an original image with the shape $256\\times256$, the base network block outputs a feature map with the shape $32 \\times 32$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnS0ULqD-OME",
        "outputId": "80700b4a-0a21-4f41-c682-69c60d91cb3e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 64, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "def base_net():\n",
        "    blk = []\n",
        "    num_filters = [3, 16, 32, 64]\n",
        "    for i in range(len(num_filters) - 1):\n",
        "        blk.append(down_sample_blk(num_filters[i], num_filters[i+1]))\n",
        "    blk = nn.Sequential(*blk)\n",
        "    return blk\n",
        "\n",
        "forward(torch.zeros((2, 3, 256, 256)), base_net()).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qi_85m0R-OME"
      },
      "source": [
        "### The Complete Model\n",
        "\n",
        "The SSD model contains a total of five modules. Each module outputs a feature\n",
        "map used to generate anchor boxes and predict the categories and offsets of\n",
        "these anchor boxes. The first module is the base network block, modules two to\n",
        "four are height and width downsample blocks, and the fifth module is a global\n",
        "maximum pooling layer that reduces the height and width to 1. Therefore, modules\n",
        "two to five are all multiscale feature blocks shown in :numref:`fig_ssd`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "sMsknVdh-OME"
      },
      "outputs": [],
      "source": [
        "def get_blk(i):\n",
        "    if i == 0:\n",
        "        blk = base_net()\n",
        "    elif i == 1:\n",
        "        blk = down_sample_blk(64, 128)\n",
        "    elif i == 4:\n",
        "        blk = nn.AdaptiveMaxPool2d((1,1))\n",
        "    else:\n",
        "        blk = down_sample_blk(128, 128)\n",
        "\n",
        "    return blk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cNAuCBk-OME"
      },
      "source": [
        "Now, we will define the forward computation process for each module. In contrast to the previously-described convolutional neural networks, this module not only returns feature map `Y` output by convolutional computation, but also the anchor boxes of the current scale generated from `Y` and their predicted categories and offsets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "K69SmwON-OMF"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import math\n",
        "def create_anchors(feature_map_sizes, steps, sizes):\n",
        "    \"\"\"Compute default box sizes with scale and aspect transform.\"\"\"\n",
        "    scale = 256.\n",
        "    steps = [s / scale for s in steps]\n",
        "    sizes = [s / scale for s in sizes]\n",
        "\n",
        "    aspect_ratios = ((2,),)\n",
        "\n",
        "\n",
        "    num_layers = len(feature_map_sizes)\n",
        "\n",
        "    boxes = []\n",
        "    for i in range(num_layers):\n",
        "        fmsize = feature_map_sizes[i]\n",
        "        for h, w in itertools.product(range(fmsize), repeat=2):\n",
        "            cx = (w + 0.5)*steps[i]\n",
        "            cy = (h + 0.5)*steps[i]\n",
        "            s = sizes[i]\n",
        "            boxes.append((cx, cy, s, s))\n",
        "\n",
        "            s = sizes[i+1]\n",
        "            boxes.append((cx, cy, s, s))\n",
        "\n",
        "            s = sizes[i]\n",
        "            for ar in aspect_ratios[i]:\n",
        "\n",
        "#                 boxes.append((cx - (s * math.sqrt(ar))/2, cy - (s / math.sqrt(ar))/2, cx + (s * math.sqrt(ar))/2, cy + (s / math.sqrt(ar))/2))\n",
        "#                 boxes.append((cx - (s / math.sqrt(ar))/2, cy - (s * math.sqrt(ar))/2, cx + (s / math.sqrt(ar))/2, cy + (s * math.sqrt(ar))/2))\n",
        "\n",
        "                boxes.append((cx, cy, (s * math.sqrt(ar)), (s / math.sqrt(ar))))\n",
        "                boxes.append((cx, cy, (s / math.sqrt(ar)), (s * math.sqrt(ar))))\n",
        "\n",
        "    return torch.Tensor(boxes) # [8632, 4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "eo5M2fr6-OMF"
      },
      "outputs": [],
      "source": [
        "def blk_forward(X, blk, size, ratio, cls_predictor, bbox_predictor):\n",
        "    Y = blk(X)\n",
        "    anchors = create_anchors((Y.size(2),), (256/Y.size(2),), size)\n",
        "    cls_preds = cls_predictor(Y)\n",
        "    bbox_preds = bbox_predictor(Y)\n",
        "    return (Y, anchors, cls_preds, bbox_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0W2R9bIH-OMF"
      },
      "source": [
        "As we mentioned, the closer a multiscale feature block is to the top in :numref:`fig_ssd`, the larger the objects it detects and the larger the anchor boxes it must generate. Here, we first divide the interval from 0.2 to 1.05 into five equal parts to determine the sizes of smaller anchor boxes at different scales: 0.2, 0.37, 0.54, etc. Then, according to $\\sqrt{0.2 \\times 0.37} = 0.272$, $\\sqrt{0.37 \\times 0.54} = 0.447$, and similar formulas, we determine the sizes of larger anchor boxes at the different scales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZIg4kjwm-OMF"
      },
      "outputs": [],
      "source": [
        "sizes = [[0.2*256, 0.272*256], [0.37*256, 0.447*256], [0.54*256, 0.619*256],\n",
        "         [0.71*256, 0.79*256], [0.88*256, 0.961*256]]\n",
        "ratios = [[1, 2, 0.5]] * 5\n",
        "num_anchors = len(sizes[0]) + len(ratios[0]) - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkOF5SL0-OMF"
      },
      "source": [
        "Now, we can define the complete model, `TinySSD`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wUPA4t8d-OMF"
      },
      "outputs": [],
      "source": [
        "class TinySSD(nn.Module):\n",
        "    def __init__(self, input_channels, num_classes):\n",
        "        super(TinySSD, self).__init__()\n",
        "\n",
        "        input_channels_cls = 128\n",
        "        input_channels_bbox = 128\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.blk = []\n",
        "        self.cls = []\n",
        "        self.bbox = []\n",
        "\n",
        "        self.blk_0 = get_blk(0)\n",
        "        self.blk_1 = get_blk(1)\n",
        "        self.blk_2 = get_blk(2)\n",
        "        self.blk_3 = get_blk(3)\n",
        "        self.blk_4 = get_blk(4)\n",
        "\n",
        "        self.cls_0 = cls_predictor(64, num_anchors, num_classes)\n",
        "        self.cls_1 = cls_predictor(input_channels_cls, num_anchors, num_classes)\n",
        "        self.cls_2 = cls_predictor(input_channels_cls, num_anchors, num_classes)\n",
        "        self.cls_3 = cls_predictor(input_channels_cls, num_anchors, num_classes)\n",
        "        self.cls_4 = cls_predictor(input_channels_cls, num_anchors, num_classes)\n",
        "\n",
        "        self.bbox_0 = bbox_predictor(64, num_anchors)\n",
        "        self.bbox_1 = bbox_predictor(input_channels_bbox, num_anchors)\n",
        "        self.bbox_2 = bbox_predictor(input_channels_bbox, num_anchors)\n",
        "        self.bbox_3 = bbox_predictor(input_channels_bbox, num_anchors)\n",
        "        self.bbox_4 = bbox_predictor(input_channels_bbox, num_anchors)\n",
        "\n",
        "    def forward(self, X):\n",
        "        anchors, cls_preds, bbox_preds = [None] * 5, [None] * 5, [None] * 5\n",
        "\n",
        "        X, anchors[0], cls_preds[0], bbox_preds[0] = blk_forward(X, self.blk_0, sizes[0], ratios[0],\n",
        "                                                                    self.cls_0, self.bbox_0)\n",
        "\n",
        "        X, anchors[1], cls_preds[1], bbox_preds[1] = blk_forward(X, self.blk_1, sizes[1], ratios[1],\n",
        "                                                                    self.cls_1, self.bbox_1)\n",
        "\n",
        "        X, anchors[2], cls_preds[2], bbox_preds[2] = blk_forward(X, self.blk_2, sizes[2], ratios[2],\n",
        "                                                                    self.cls_2, self.bbox_2)\n",
        "\n",
        "        X, anchors[3], cls_preds[3], bbox_preds[3] = blk_forward(X, self.blk_3, sizes[3], ratios[3],\n",
        "                                                                    self.cls_3, self.bbox_3)\n",
        "\n",
        "        X, anchors[4], cls_preds[4], bbox_preds[4] = blk_forward(X, self.blk_4, sizes[4], ratios[4],\n",
        "                                                                    self.cls_4, self.bbox_4)\n",
        "\n",
        "        return (torch.cat(anchors, dim=0), concat_preds(cls_preds).reshape((-1, 5444, self.num_classes + 1)),\n",
        "                concat_preds(bbox_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "gDqWOJZs-OMF"
      },
      "outputs": [],
      "source": [
        "anchors, cls_preds, bbox_preds = [None] * 5, [None] * 5, [None] * 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vquJBJo-OMF"
      },
      "source": [
        "We now create an SSD model instance and use it to perform forward computation on image mini-batch `X`, which has a height and width of 256 pixels. As we verified previously, the first module outputs a feature map with the shape $32 \\times 32$. Because modules two to four are height and width downsample blocks, module five is a global pooling layer, and each element in the feature map is used as the center for 4 anchor boxes, a total of $(32^2 + 16^2 + 8^2 + 4^2 + 1)\\times 4 = 5444$ anchor boxes are generated for each image at the five scales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vc7Rj01y-OMF",
        "outputId": "c7871286-f86e-42b6-e32b-a13de46d69d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output anchors: torch.Size([5444, 4])\n",
            "output class preds: torch.Size([32, 5444, 2])\n",
            "output bbox preds: torch.Size([32, 21776])\n"
          ]
        }
      ],
      "source": [
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "net = TinySSD(3, num_classes=1)\n",
        "net.apply(init_weights)\n",
        "\n",
        "X = torch.zeros((32, 3, 256, 256))\n",
        "anchors, cls_preds, bbox_preds = net(X)\n",
        "\n",
        "print('output anchors:', anchors.shape)\n",
        "print('output class preds:', cls_preds.shape)\n",
        "print('output bbox preds:', bbox_preds.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYS0hU2F-OMF"
      },
      "source": [
        "## Training\n",
        "\n",
        "Now, we will explain, step by step, how to train the SSD model for object detection.\n",
        "\n",
        "### Data Reading and Initialization\n",
        "\n",
        "We read the Pikachu data set we created in the previous section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2VjvFjQ-OMF"
      },
      "source": [
        "For this part you need mxnet library to download and convert the mxnet's .rec file to png images and they will be saved in the 'data' directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rB18PNoW-OMG"
      },
      "outputs": [],
      "source": [
        "d2l.download_and_preprocess_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXFkB_z--OMG",
        "outputId": "9fe40174-e771-4a5a-b74b-e8b7ef2409c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "data_dir = '../data/pikachu/'\n",
        "train_dataset = d2l.PIKACHU(data_dir, 'train')\n",
        "val_dataset = d2l.PIKACHU(data_dir, 'val')\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                           batch_size=batch_size, shuffle=True,\n",
        "                                           num_workers=4)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                                         batch_size=batch_size, shuffle=False,\n",
        "                                         num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5LGBjiE-OMG"
      },
      "source": [
        "There is 1 category in the Pikachu data set. After defining the module, we need to initialize the model parameters and define the optimization algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "AI_CcS-c-OMG"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '7' # You can set gpu no. if you have multiple gpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lr8sAukn-OMG",
        "outputId": "50775ce0-e1b3-4f18-fb79-3f0aa577273d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5m8MSwJX-OMG"
      },
      "outputs": [],
      "source": [
        "net = TinySSD(3, num_classes=1)\n",
        "net.apply(init_weights)\n",
        "net = net.to(device)\n",
        "\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 5e-4\n",
        "optimizer = optim.SGD(net.parameters(), lr = learning_rate, weight_decay=weight_decay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGt8rl6c-OMG"
      },
      "source": [
        "### Define Loss and Evaluation Functions\n",
        "\n",
        "Object detection is subject to two types of losses. The first is anchor box category loss. For this, we can simply reuse the cross-entropy loss function we used in image classification. The second loss is positive anchor box offset loss. Offset prediction is a normalization problem. However, here, we do not use the squared loss introduced previously. Rather, we use the $L_1$ norm loss, which is the absolute value of the difference between the predicted value and the ground-truth value.Then we remove the negative anchor boxes and padding anchor boxes from the loss calculation. Finally, we add the anchor box category and offset losses to find the final loss function for the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1lm1Blr-OMG"
      },
      "source": [
        "We can use the accuracy rate to evaluate the classification results. As we use the $L_1$ norm loss, we will use the average absolute error to evaluate the bounding box prediction results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "VO6kxfRT-OMG"
      },
      "outputs": [],
      "source": [
        "id_cat = dict()\n",
        "id_cat[0] = 'pikachu'\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2, device=\"cuda:0\", eps=1e-10):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.device = device\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        p = torch.sigmoid(input)\n",
        "        pt = p * target.float() + (1.0 - p) * (1 - target).float()\n",
        "        alpha_t = (1.0 - self.alpha) * target.float() + self.alpha * (1 - target).float()\n",
        "        loss = - 1.0 * torch.pow((1 - pt), self.gamma) * torch.log(pt + self.eps)\n",
        "        return loss.sum()\n",
        "\n",
        "class SSDLoss(nn.Module):\n",
        "    def __init__(self, loc_factor, jaccard_overlap, device = \"cuda:0\", **kwargs):\n",
        "        super().__init__()\n",
        "        self.fl = FocalLoss(**kwargs)\n",
        "        self.loc_factor = loc_factor\n",
        "        self.jaccard_overlap = jaccard_overlap\n",
        "\n",
        "\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "    def one_hot_encoding(labels, num_classes):\n",
        "        return torch.eye(num_classes)[labels]\n",
        "\n",
        "    def loc_transformation(x, anchors, overlap_indicies):\n",
        "        # Doing location transformations according to SSD paper\n",
        "        return torch.cat([(x[:, 0:1] - anchors[overlap_indicies, 0:1]) / anchors[overlap_indicies, 2:3],\n",
        "                          (x[:, 1:2] - anchors[overlap_indicies, 1:2]) / anchors[overlap_indicies, 3:4],\n",
        "                          torch.log((x[:, 2:3] / anchors[overlap_indicies, 2:3])),\n",
        "                          torch.log((x[:, 3:4] / anchors[overlap_indicies, 3:4]))\n",
        "                         ], dim=1)\n",
        "\n",
        "    def forward(self, class_hat, bb_hat, class_true, bb_true, anchors):\n",
        "        loc_loss = 0.0\n",
        "        class_loss = 0.0\n",
        "\n",
        "\n",
        "        for i in range(len(class_true)):  # Batch level\n",
        "\n",
        "            class_hat_i = class_hat[i, :, :]\n",
        "\n",
        "            bb_true_i = bb_true[i].float()\n",
        "\n",
        "            class_true_i = class_true[i]\n",
        "\n",
        "            class_target = torch.zeros(class_hat_i.shape[0]).long().to(self.device)\n",
        "\n",
        "            overlap_list = d2l.find_overlap(bb_true_i.squeeze(0), anchors, self.jaccard_overlap)\n",
        "\n",
        "            temp_loc_loss = 0.0\n",
        "            for j in range(len(overlap_list)):  # BB level\n",
        "                overlap = overlap_list[j]\n",
        "                class_target[overlap] = class_true_i[0, j].long()\n",
        "\n",
        "                input_ = bb_hat[i, overlap, :]\n",
        "                target_ = SSDLoss.loc_transformation(bb_true_i[0, j, :].expand((len(overlap), 4)), anchors, overlap)\n",
        "\n",
        "                temp_loc_loss += F.smooth_l1_loss(input=input_, target=target_, reduction=\"sum\") / len(overlap)\n",
        "            loc_loss += temp_loc_loss / class_true_i.shape[1]\n",
        "\n",
        "            class_target = SSDLoss.one_hot_encoding(class_target, len(id_cat) + 1).float().to(self.device)\n",
        "            class_loss += self.fl(class_hat_i, class_target) / class_true_i.shape[1]\n",
        "\n",
        "        loc_loss = loc_loss / len(class_true)\n",
        "        class_loss = class_loss / len(class_true)\n",
        "        loss = class_loss + loc_loss * self.loc_factor\n",
        "\n",
        "        return loss, loc_loss, class_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOgxYvka-OMK"
      },
      "source": [
        "### Train the Model\n",
        "\n",
        "During model training, we must generate multiscale anchor boxes (`anchors`) in the model's forward computation process and predict the category (`cls_preds`) and offset (`bbox_preds`) for each anchor box.  Finally, we calculate the loss function using the predicted and labeled category and offset values. To simplify the code, we do not evaluate the training data set here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "5GX9emNl-OMK"
      },
      "outputs": [],
      "source": [
        "loss = SSDLoss(loc_factor=5.0, jaccard_overlap=0.5, device=\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "KrK_uSn8-OMK",
        "outputId": "dea43842-23e1-4c99-916b-14dc3259a71d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-f675c864b575>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0manchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_loc_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_class_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbb_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-ee91d1446d59>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, class_hat, bb_hat, class_true, bb_true, anchors)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mloc_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtemp_loc_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mclass_true_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mclass_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSSDLoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_cat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mclass_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_hat_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_target\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mclass_true_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-ee91d1446d59>\u001b[0m in \u001b[0;36mone_hot_encoding\u001b[0;34m(labels, num_classes)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mone_hot_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloc_transformation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverlap_indicies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: indices should be either on cpu or on the same device as the indexed tensor (cpu)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 350x250 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"240.554688pt\" height=\"173.477344pt\" viewBox=\"0 0 240.554688 173.477344\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2023-11-15T03:17:43.828999</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 173.477344 \nL 240.554688 173.477344 \nL 240.554688 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 149.599219 \nL 225.403125 149.599219 \nL 225.403125 10.999219 \nL 30.103125 10.999219 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"mf8503c9653\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mf8503c9653\" x=\"30.103125\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0.0 -->\n      <g transform=\"translate(22.151563 164.197656) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#mf8503c9653\" x=\"69.163125\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 0.2 -->\n      <g transform=\"translate(61.211563 164.197656) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#mf8503c9653\" x=\"108.223125\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0.4 -->\n      <g transform=\"translate(100.271563 164.197656) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#mf8503c9653\" x=\"147.283125\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0.6 -->\n      <g transform=\"translate(139.331563 164.197656) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#mf8503c9653\" x=\"186.343125\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0.8 -->\n      <g transform=\"translate(178.391563 164.197656) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#mf8503c9653\" x=\"225.403125\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 1.0 -->\n      <g transform=\"translate(217.451563 164.197656) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path id=\"mec776e4e13\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mec776e4e13\" x=\"30.103125\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.0 -->\n      <g transform=\"translate(7.2 153.398438) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#mec776e4e13\" x=\"30.103125\" y=\"121.879219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.2 -->\n      <g transform=\"translate(7.2 125.678438) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#mec776e4e13\" x=\"30.103125\" y=\"94.159219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.4 -->\n      <g transform=\"translate(7.2 97.958438) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#mec776e4e13\" x=\"30.103125\" y=\"66.439219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.6 -->\n      <g transform=\"translate(7.2 70.238438) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use xlink:href=\"#mec776e4e13\" x=\"30.103125\" y=\"38.719219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.8 -->\n      <g transform=\"translate(7.2 42.518438) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#mec776e4e13\" x=\"30.103125\" y=\"10.999219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 1.0 -->\n      <g transform=\"translate(7.2 14.798438) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 149.599219 \nL 30.103125 10.999219 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 225.403125 149.599219 \nL 225.403125 10.999219 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 149.599219 \nL 225.403125 149.599219 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 10.999219 \nL 225.403125 10.999219 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n </g>\n</svg>\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "num_epochs = 25\n",
        "init_epoch = 0\n",
        "\n",
        "# Uncomment the following 2 lines if you wish to load a pre-trained/saved model\n",
        "# checkpoint_path = './ssd_outputs/model-29_0.1411931432526687.pth'   # Mention the model name to load\n",
        "# init_epoch = d2l.load(net, checkpoint_path, optimizer)\n",
        "\n",
        "animator = d2l.Animator(xlabel='epoch', xlim=[init_epoch+1, num_epochs],\n",
        "                        legend=['class error', 'bbox mae', 'train_err'])\n",
        "\n",
        "for epoch in range(init_epoch, num_epochs):\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    train_loss = 0.0\n",
        "    loc_loss = 0.0\n",
        "    class_loss = 0.0\n",
        "\n",
        "    for i, (x, bb_true, class_true) in (enumerate(train_loader)):\n",
        "\n",
        "        x = x.to(device)\n",
        "        bb_true = bb_true.to(device)\n",
        "        class_true = class_true.to(device)\n",
        "\n",
        "        timer_start = time.time()\n",
        "\n",
        "        anchors, cls_preds, bbox_preds = net(x)\n",
        "\n",
        "        class_true = [*class_true.reshape((class_true.size(0), 1, 1))]\n",
        "        bb_true = [*bb_true.reshape((bb_true.size(0), 1, 1, 4))]\n",
        "\n",
        "        bbox_preds = bbox_preds.reshape((-1, 5444, 4))\n",
        "\n",
        "        # Label the category and offset of each anchor box\n",
        "\n",
        "        anchors = anchors.to(device)\n",
        "\n",
        "        batch_loss, batch_loc_loss, batch_class_loss = loss(cls_preds, bbox_preds, class_true, bb_true, anchors)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        class_loss += batch_class_loss\n",
        "        loc_loss += batch_loc_loss\n",
        "        train_loss += batch_loss\n",
        "\n",
        "#     Uncomment the following 2 line for saving the model every epoch\n",
        "#     path_to_checkpoints_dir = './ssd_outputs/'\n",
        "#     d2l.save(net, path_to_checkpoints_dir, epoch, optimizer, train_loss/len(train_loader))\n",
        "\n",
        "    train_loss = (train_loss/len(train_loader)).detach().cpu().numpy()\n",
        "    loc_loss = (loc_loss/len(train_loader)).detach().cpu().numpy()\n",
        "    class_loss = (class_loss/len(train_loader)).detach().cpu().numpy()\n",
        "\n",
        "#     print(class_loss, loc_loss, train_loss, epoch+1)\n",
        "\n",
        "    # Uncomment the following if you wish to see the results after every epoch to see the learning effect\n",
        "    # Images will be saved to the 'results_every_epoch' directory\n",
        "\n",
        "#     try:\n",
        "#         d2l.infer(net, epoch, 0.9, device)\n",
        "#     except Exception as e:\n",
        "#         print(e, 'error' + str(epoch+1))\n",
        "\n",
        "\n",
        "    animator.add(epoch, (class_loss, loc_loss, train_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnjidmb6-OMK"
      },
      "source": [
        "### Saving the final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F13om9KV-OMK"
      },
      "outputs": [],
      "source": [
        "path_to_checkpoints_dir = './ssd_outputs'\n",
        "d2l.save(net, path_to_checkpoints_dir, epoch, optimizer, train_loss/len(train_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt2LqADw-OMK"
      },
      "source": [
        "## Prediction\n",
        "\n",
        "In the prediction stage, we want to detect all objects of interest in the image. Below, we read the test image and transform its size. Then, we convert it to the four-dimensional format required by the convolutional layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCbUfd62-OMK"
      },
      "outputs": [],
      "source": [
        "img = np.array(Image.open('../img/pikachu.jpg').convert('RGB').resize((256, 256), Image.BILINEAR))\n",
        "X = transforms.Compose([transforms.ToTensor()])(img).to(device)\n",
        "X = X.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGp73Ryc-OML"
      },
      "source": [
        "We predict the bounding boxes based on the anchor boxes and their predicted offsets. Then, we use non-maximum suppression to remove similar bounding boxes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYbWE9zx-OML"
      },
      "outputs": [],
      "source": [
        "def predict(X, nms_threshold):\n",
        "    background_threshold = 0.8\n",
        "    net.eval()\n",
        "    anchors, class_hat, bb_hat = net(X.unsqueeze(0))\n",
        "    anchors = anchors.to(device)\n",
        "    bb_hat = bb_hat.reshape((1, -1, 4))\n",
        "    bb_hat = d2l.invert_transformation(bb_hat.squeeze(0), anchors)\n",
        "    bb_hat = bb_hat * 256.0\n",
        "\n",
        "    class_hat = class_hat.sigmoid().squeeze(0)\n",
        "\n",
        "    bb_hat = bb_hat[class_hat[:,0] < background_threshold, :]\n",
        "\n",
        "\n",
        "    bb_hat = bb_hat.detach().cpu().numpy()\n",
        "    class_hat = class_hat[class_hat[:,0] < background_threshold, :]\n",
        "\n",
        "    class_preds = class_hat[:, 1:]\n",
        "\n",
        "    prob, class_id = torch.max(class_preds,1)\n",
        "\n",
        "    prob = prob.detach().cpu().numpy()\n",
        "    class_id = class_id.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "\n",
        "    output_bb = [d2l.PredBoundingBox(probability=1 - prob[i],\n",
        "                                 class_id=class_id[i],\n",
        "                                 classname=id_cat[class_id[i]],\n",
        "                                 bounding_box=[bb_hat[i, 0],\n",
        "                                               bb_hat[i, 1],\n",
        "                                               bb_hat[i, 2],\n",
        "                                               bb_hat[i, 3]])\n",
        "                                 for i in range(0, len(prob))]\n",
        "\n",
        "    output_bb = sorted(output_bb, key = lambda x: x.probability, reverse=False)\n",
        "\n",
        "    filtered_bb = d2l.non_max_suppression(output_bb, nms_threshold)\n",
        "\n",
        "    return filtered_bb\n",
        "\n",
        "filtered_bb = predict(X, 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0MQudSA-OML"
      },
      "source": [
        "Finally, we take all the bounding boxes and display them as the final output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "tRuKzn8Z-OML"
      },
      "outputs": [],
      "source": [
        "def display(img, output):\n",
        "\n",
        "    img = d2l.draw_boxes(img, [bb.bounding_box for bb in filtered_bb])\n",
        "    img = d2l.draw_text(img, [str(bb.probability)[:5] for bb in filtered_bb], [bb.bounding_box for bb in filtered_bb])\n",
        "    d2l.plt.imshow(img)\n",
        "    d2l.plt.show()\n",
        "\n",
        "display(img, filtered_bb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr9bxpym-OML"
      },
      "source": [
        "## Summary\n",
        "\n",
        "* SSD is a multiscale object detection model. This model generates different numbers of anchor boxes of different sizes based on the base network block and each multiscale feature block and predicts the categories and offsets of the anchor boxes to detect objects of different sizes.\n",
        "* During SSD model training, the loss function is calculated using the predicted and labeled category and offset values.\n",
        "\n",
        "\n",
        "\n",
        "## Exercises\n",
        "\n",
        "* Due to space limitations, we have ignored some of the implementation details of SSD models in this experiment. Can you further improve the model in the following areas?\n",
        "\n",
        "\n",
        "### Loss Function\n",
        "\n",
        "For the predicted offsets, replace $L_1$ norm loss with $L_1$ regularization loss. This loss function uses a square function around zero for greater smoothness. This is the regularized area controlled by the hyper-parameter $\\sigma$:\n",
        "\n",
        "$$\n",
        "f(x) =\n",
        "    \\begin{cases}\n",
        "    (\\sigma x)^2/2,& \\text{if }|x| < 1/\\sigma^2\\\\\n",
        "    |x|-0.5/\\sigma^2,& \\text{otherwise}\n",
        "    \\end{cases}\n",
        "$$\n",
        "\n",
        "When $\\sigma$ is large, this loss is similar to the $L_1$ norm loss. When the value is small, the loss function is smoother."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nosFjfXP-OML"
      },
      "outputs": [],
      "source": [
        "sigmas = [10, 1, 0.5]\n",
        "lines = ['-', '--', '-.']\n",
        "x = np.arange(-2, 2, 0.1)\n",
        "d2l.set_figsize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqBoOMG--OML"
      },
      "outputs": [],
      "source": [
        "def smooth_l1(x, scalar):\n",
        "    a = []\n",
        "    for i in x:\n",
        "\n",
        "        if abs(i) < 1/((scalar)**2):\n",
        "            a.append(((scalar*i)**2)/2)\n",
        "\n",
        "        else:\n",
        "            a.append(abs(i) - 0.5/((scalar)**2))\n",
        "\n",
        "    return np.array(a)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCT23qew-OMM"
      },
      "outputs": [],
      "source": [
        "a1 = []\n",
        "for l, s in zip(lines, sigmas):\n",
        "    y = smooth_l1(x, scalar=s)\n",
        "    d2l.plt.plot(x, y, l, label='sigma=%.1f' % s)\n",
        "\n",
        "d2l.plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcH8INH1-OMM"
      },
      "source": [
        "In the experiment, we used cross-entropy loss for category prediction. Now,\n",
        "assume that the prediction probability of the actual category $j$ is $p_j$ and\n",
        "the cross-entropy loss is $-\\log p_j$. We can also use the focal loss\n",
        ":cite:`Lin.Goyal.Girshick.ea.2017`. Given the positive hyper-parameters $\\gamma$\n",
        "and $\\alpha$, this loss is defined as:\n",
        "\n",
        "$$ - \\alpha (1-p_j)^{\\gamma} \\log p_j.$$\n",
        "\n",
        "As you can see, by increasing $\\gamma$, we can effectively reduce the loss when the probability of predicting the correct category is high."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "nIbLuDNq-OMM"
      },
      "outputs": [],
      "source": [
        "def focal_loss(gamma, x):\n",
        "    return -(1 - x) ** gamma * np.log(x)\n",
        "\n",
        "x = np.arange(0.01, 1, 0.01)\n",
        "\n",
        "for l, gamma in zip(lines, [0, 1, 5]):\n",
        "    y = d2l.plt.plot(x, focal_loss(gamma, x), l,\n",
        "                     label='gamma=%.1f' % gamma)\n",
        "\n",
        "d2l.plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxAqvqK0-OMN"
      },
      "source": [
        "### Training and Prediction\n",
        "\n",
        "* When an object is relatively large compared to the image, the model normally adopts a larger input image size.\n",
        "* This generally produces a large number of negative anchor boxes when labeling anchor box categories. We can sample the negative anchor boxes to better balance the data categories.\n",
        "* Assign hyper-parameters with different weights to the anchor box category loss and positive anchor box offset loss in the loss function.\n",
        "* Refer to the SSD paper. What methods can be used to evaluate the precision of\n",
        "  object detection models :cite:`Liu.Anguelov.Erhan.ea.2016`?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}